17 June 2023

Live training Videos -> 41st Batch .
AWS Students -> Slides -> AWS 3in1 Course

<>
Trusted Advisor (Consulting)
Customized Cloud Expert . Best practse for saving money , improving performace and closing gaps.
Recommends only (On by default)
5 domain (to be advised on) 
(Cost optimization)
Performance
security
Fault tolerance
Serivce limit

Limited core checks under free tier
Can be configured to notify via emails on reports
consults on security gaps on new deployments 
IAM can be used to see which user has access to which notification or security check

Obviously recommendations can be exported and van be fetched through API as well. 


Core Checks (basic plan / free plan)

Service Limit : default limit of a specific service e.g 20 EC2 machines in a specific DC
Security: S3 Bucket permission (which of the buckets are public) <> Security Groups (if anything is un-restricted) <> IAM Use case (best practises) <> MFA on root account <> EBS Public Snapshot (drive of the EC2 machines) if any of those are public <> RDS public snapshots


Full Trusted Advisor Benefits (Enterprise or Buisness)
https://aws.amazon.com/premiumsupport/plans/

<>
Cloud Trail (more like a trail of action of 90 days on the dashboard) <> On by default
History of event activity on the AWS account
including AWS management consile , through CLI or even the SDK .
Basically a configuration change audit 
Can be store in a S3 bucket (this is a manual process -> make a trail to save it in the S3 bucket)
In the above trail (ths is encrypted by SSE server side encryption) -> by default
Can use AWS KMS as well for encryption 

Log file integrity validation (this is a feature that you enable on the cloud trail)
validates if the data changed was modified or not

DATA Events (data plane) <> Additional Cost (you can see those in the trail you create)
The actual activity on the object or resourse itself . Change /update delete , upload , download etc e.g RDP (SSH) what is happening in the resource itself


Management Event (control plane) (this is something that you would see on the trail)
Account logins and object creations not anything with respect to resource. Account level activity


Cloud Trail insights (this is something that you would see on the trail)
identify unusual activity that is on a delta from your usual norm

Multi-region Configuration 
in relaity cloud trail is your regional service but you can configure AWS cloud trail to deliver log files from multiple region to a signle S3 bucket

AWS LAMBDA,
To triger an action from something in the cloud trail

AWS cloud watch


Prcing Example
90 days van be viewed for free
first trail creation is free but if you save it in S3 , S3 is not free


<>
IAM (free resource)
Identity and access management (AWS resources)
basically your AAA service
Identities and permsissions assigned to that resource (this is in a JSON file sitting somewhere)

Root User: the parent account from which it was registerd (this always have everything super admin)
IAM user : the user created under a account identity or account alias

Shared access to the same AWS parent account
can be used with MFA (better to use for privilidged users)
analyze access (basically the accounting for the IAM)

Identity Federation
AWS can also use an IDP for logins (basically an SSO)
Identity federation is a mechanism that allows you to establish trust and enable single sign-on (SSO) between different systems or services.

Cross account access (need to study)

Policies:
identity based policy: policy to a user or a group (inline policy: a policy that exists with a user, if the user gets deleted the policy gets deleted)
resource based policy: polciy that can be assinged to a resource (just like an inline , it gets deleted with the resource)
permission boundries: defines the maximum boundry of access , if any access given to a user to not mentioned in the boundry will not work
Organization SCP (service control policy): (need to study)

By using SCPs, you can define a hierarchy of permissions and constraints across your organization's AWS accounts. This helps you enforce security, compliance, and governance policies consistently across all accounts.

In summary, an SCP is a way to control what actions and services are allowed within your AWS organization's accounts. It ensures that everyone follows the rules and guidelines you set up, just like a parent managing their children's activities.


BEST practises

user: create individual user
groups: manager permission with groups
permission: grant least privilidge
auditng : turn on cloud trail
password: configure strong password
MFA: support MFA
roles: use IAM roles for amazon EC2 instance
sharing: use IAM role to share access
rotate: rotate security credentials regularly
condition: restricit privilidge access with conditions 
root: reduce or remove use of root

IAM roles and groups (Need to study)


Create a challenge password. You will need to verbally provide this password to the CA Administrator in order to revoke your certificate.




If I do not create a trail, can I still see the DATA Events and MGMT events  where can I see the ?
In identity federation , we could use an IDP to get temp access in AWS network? define temporary , can it not be just normal access? - Slide 6

Questions:
Difference between pre-signed URL and public URL
To an S3, folders don't really exist or do they?
what are object keys and prefix?



<> S3 (simple storage service)

This is a object based storage
This is where you save your objects like your files and stuff into a resource called Bucket (folder)
This is a Global service but your region gets created in a region but you can still view them in all other regions in the S3
Data format is ir-relevant . File sizes are 0 Byte to 5 TB. 
Files with more than 5GB or 100 MB as per Rafay in size must use "multi-part upload"

objects can have metadata , which would be a list of text key/value pairs - system or user metadata)



All objects in S3 have keys that would be it's path from the S3 bucket
e.g s3://my-bucket/my_folder1/another_folder/my_file.txt

Department: Eng.


key = prefix + object name
where your folder directories are your prefix e.g here my_folder1/another_folder/


Charges on S3 are for Storage, Data Transfer and Acceleration (This is a sepearate feature where you can file to the edge location)
S3 Transfer Acceleration = When you upload or download a file, your data is automatically routed through the nearest edge location.

S3 bucket name must be unique globally , meaning I can not have a bucket-name that has the same name as somewhere else in the world

Storage Classes (diffrentiated by cost and access)
1) S3 Standard - for your normal BAU (buisness as usual) usage (default) - most expensive as well - No retreival fees. On back end it is backed up in at least 3-AZ
2) S3 Standard IA (in-frequent access) - On back end it is backed up in at least 3-AZ - slightly cheaper - has a retreival fee (slightly less frequent data but instantly available)
3) S3 One Zone - infrequent access - Exactly as you Standard IS but your data stored is NOT backed up to any place 
4) S3 Glacier (Archive)

In frequent Access Tier
S3 Standard IA
S3 One Zone - infrequent access

Archive Tier
Glacier Instant Retrieval: 
lower cost but highest in archive (glacier) with long lived data rarely accesses , retival in miliseconds.

Glacrier Flexible Retrival 
cheaper than above but can be retrieved in Expedited (1-5 minutes) - Standard (3-5 Hours) - Bulk (5 - 12 Hours)
Provisioned capacity unit (fixed up-front fee to expedire retrievals ) - need to study - really study (Need to confirm with Rafay)


Glacier Deep Archive for years of Standard
storage (within 12 hours)
Bulk (up to 48 hours)


Unknown or Changing Access

S3 Intellegient Tiering
Optimizes cost by automatically moving data to the most cost effective access tier
Does this by monitoring your frequent access tier for 30 consecutive days
There is a per object monitoring fees
How does it move objects back to frequent access when it becoes more accessible?
S3 Intelligent-Tiering is a storage class in AWS S3 that automatically optimizes the cost and performance of storing your data by moving objects between frequent access and infrequent access tiers based on their usage patterns.


It has 3 tiers in consideration
1: optimized for frequent access (default)
2: low cost tier optimized for infrequent access (zero request in last 30 days)
3: Archive for rare access (zero request in last 90 days from IA)




S3 outpost 
storage on premisis
for low latency to on-prem systems , basically you get a rack that you get deploy on your own DC and then it connects directly to the cloud
This is not at all different to the cloud
best of local data processing, low latency 


Storage management
S3 - Batch Operation (need to study)
S3 Batch Operations is a feature provided by Amazon S3 that allows you to perform actions on a large number of files in a single request, saving time and effort. It simplifies the management and processing of a large-scale file operations in S3.


S3 Versioning
creates  a version of the file everytime the file is modified
e.g so you changed it to 10 times so now you would have 11 different files
Disabling (suspending) versioning does not delete your previous versions
Delete Marker (need to study)
Any file that is not versioned prior to enabling versioning will have the version "null"
Deleting the latest version will make the second last be the current version of the file

same key overwrite will change the version of the file.
just like you do in git

In Amazon S3 versioning, when you delete a file, instead of immediately erasing it completely, a "delete marker" is created. Think of it like a signpost that shows the file has been deleted.

Yes, you can think of deleting a delete marker in Amazon S3 versioning as similar to restoring an item from the recycle bin on a Windows PC.

Yes, deleting a file in Amazon S3 when it already has a delete marker associated with it will result in the complete and permanent deletion of that object but but if you have a previous version and your delete marker is on the latest version in that case it would restore the second last version into the bucket. Because in reality the previous version was suppressed by the delete marker on the latest version


Multi Factor authentication Delete
helps protect your data stored in an Amazon S3 bucket from accidental deletion.
you also need a special code from a physical device called an MFA device.
you need MFA for deletion of file as well in S3. MFA delete is a security feature that requires users to enter a one-time passcode from their MFA device in order to delete a versioned object in an S3 bucket. (Need to confirm with Rafay)


S3 object Lock (WORM Logic)
Object in the S3 would be locked , write once , read many

Replication
Only new files are repliced after enabling replication
options : use batch replication (this will replicate existing objects and objects that failed replication)
Delete markers can be replicated : optional
deletion with a version ID is not replicated
Replication is not chained.
E.g if bucket 1 has replication into bucket 2 which is replicated into backet 3.



S3 CRR (cross region replication) - use case fr compliance and replication accross accounts)
this allows your data to be replicated in other regions as well , basically replicats all your stuff and keeps a copy in another region.
Replication is asynchronous and versioning must be enabled.



When you have a file and you enable CRR , now you have a total of 6 copies of it?

No, if you have a file in S3 and enable CRR, there will not be 6 copies of the file on AWS. CRR only creates two copies of the file, one in the source bucket and one in the destination bucket.
The source bucket is the bucket where the file is originally stored. The destination bucket is the bucket where the file is replicated to. The two buckets can be in different AWS Regions or in the same Region.
CRR is a fully-managed service, so you don't have to worry about managing the replication process. Amazon S3 will automatically replicate the file to the destination bucket as soon as it is written to the source bucket.

SRR - Same region replication
(lowaggregaton , live replication between test and prod )

AWS cost Allocation reports
Can apply tags to an S3 bucket , and then do a reporting later on that specific tag = max value of 10 



AWS storage class analysis
Storage class analysis in Amazon S3 is a feature that helps you analyze the access patterns of your S3 objects. It provides insights into the object access frequencies and suggests potential cost savings by recommending suitable storage classes for your objects

Storage class analysis in Amazon S3 is a feature that helps you analyze the access patterns of your S3 objects. It provides insights into the object access frequencies and suggests potential cost savings by recommending suitable storage classes for your objects. By analyzing the access patterns, you can make informed decisions about optimizing storage costs based on the frequency of object access.




You can set metrics on the cloudwatch to monitor S3
can do cloud trail as well on the S3
can do event notification on the S3 as well and obvoiusly an action on that process is called lambda


Access Management
can be managed by IAM

ACL : ACLs applied on the objects

Bucket policies : to configure permission for all objects within a single bucket. Those are not your global S3 policies . There are more granular based on a specific bucket . This is reource based policy.
The policies that you create on IAM is for all buckets collectively. This is for a specific user. Also used for cross account access

Principal : the account or user to apply the policy to 
actions: set of API to allow or deny e.g Get object
Effect: allow or deny

Example : Public Access - use bucket policy because you can not define an IAM policy for this
Create a bucket policy to view objects , attach bucket policy to the 

Example: User to Access S3
create IAM policy for the IAM user

Example EC2 acces 
Createa  a new EC2 instance role with IAM permission 

Cross-Account-Access
Create bucket polcy for the bucket to include cross account access



Object Access Control list (Object ACL) -> They define access permissions for specific objects, allowing you to control who can read, write, or delete each object.
Bucket ACL 
Bucket ACLs are applied at the bucket level.
They determine the default access permissions for all the objects within a bucket.
Bucket ACLs can be used to control who can perform operations on the bucket, such as listing objects, uploading new objects, or deleting objects.
When an object is uploaded to a bucket, it inherits the default access permissions defined by the bucket's ACL.





Presigned URL 
A URL or something against an object that gets expired after a spefic length of time but this is not your public URL
Presigned URL in AWS, you can generate a time-limited link that allows someone to access a file in your S3 bucket without requiring them to have AWS credentials. The URL is like a temporary key that provides secure access to the file for a limited time period.


Audit logs
These are a sort of trails but for a specific bucket only 





S3 will automatically encrypt

options
SSE-S3 amazon S3 will encrypt the data at rest and manager the encryption keys
SSE-C the same with customer provided keys
SSE-KMS with amazon KMS : encryption with amazon provided keys


S3 inventory
inventory pull out for all your resources and where they are , in a CSV or ORC 

Amazon Macie
Amazon Macie is a service provided by Amazon Web Services (AWS) that uses advanced machine learning algorithms to automatically discover, classify, and protect sensitive data such as personally identifiable information (PII), credit card numbers, or intellectual property. 

This does it by discover , classify and protect method


S3 select:
S3 Select is a feature provided by Amazon S3 that enables you to retrieve a specific portion of data from an object (file) stored in S3, instead of downloading the entire file. It's like searching for a specific sentence or paragraph in a book without having to read the entire book.


Transfer Acceleration (not enabled by default)
S3 Transfer Acceleration = When you upload or download a file, your data is automatically routed through the nearest edge location.

Lifecycle Rule:
Exactly as intellegent Tiering but the only difference is that days and class storages are configured manually to be changed



CORS (cross origin resource sharing) (need to study) (Need to confirm with Rafay)
Let's say you have an S3 bucket named "my-bucket" where you store various files, such as images and documents. You have two websites: "example.com" and "mywebsite.net," each hosted on different domains.

To allow "example.com" and "mywebsite.net" to access resources in your "my-bucket" S3 bucket, you would configure CORS rules on the bucket and specify the allowed origins as "example.com" and "mywebsite.net". This allows both domains to make cross-origin requests to access resources in the same S3 bucket.

So, different domains can be granted access to the same S3 bucket by configuring CORS rules, enabling those websites to interact with the resources (files) stored in the bucket.



Cloud Front
Basically a CDN that uses edge locations to get you data faster


1. Devaloper puts the data into origion (can be any source , server or bucket)
2. Devaloper creates a web distribution
3. Devaloper gets a link
4. Link gets shared to end use


Version updating is automatically cashed at location. That means you can keep changing the object in the origin


Origin can be another AWS service as well.
Origin Failover: in case your origin fails off , the user can still be served via an edge location that has the last cache
Regional edge cache: another hope in the caching train. just to speed stuff up





Questions:

To an S3, folders don't really exist or do they?
what are object keys and prefix?
what is meant by a multi-part upload ? is it for file size above 5TB?
What is a cloud front signed URL?
is S3 bucket not always encrypted?





<> Cloud Watch

For createing matrics on the resources
This is a regional service : Will only work on statistics for the region itself is in.
but multiple attributes from different regions can be pinned in the dashboard 
Dashboard is not free  and is global

Basic monitoring is free , which is near real-time
Detailed monitoring is not free , and can work on 1 min refresh rate as well


Collect: Allows to collect logs from resources and applications
Need to install log agent on the resource

Monitor: visualize your cloud resources and applications in unified view   
Act
Event Bridge or action on an event








Cloud Wathch monitoring Actions

1 Auto scaling
2 SNS (simple notification )
3 EC2 Action
4 System Manager Ops Center (system related )


Metric

Namespace = name of the service (EC2 or Autoscaling , ELB etc)
Dimention: is a name/value pair that yu uniquely identify a metric e.g goup name etc.
Time Stamp : specific time interval where data points exist (X-axis of the graph)
Units: Data point value at a specific time interval (y-axis of the graph)

Renetition
data points for 60 second internval data is available for 3 hours
data points for a period of 1 min internval available for 15 days
data points less than 5 min internval data is available for 63 days
data points less than 1 hour internval data is available for 455 days


The data is not lost but instead aggregate. E.g after 3 hours , the data is merged with data that you see in 1 min


<> VPC
max limit per region = 5 (soft limit can be increased)
Regional resource 
Virtual private cloud
virtual network on the AWS - you decide your addresses / subnet etc
by default there is one 

Components

1) CIDR Block (multiple can be added using the secondary option) - maximum is a /16
2) Subnet
3) Gateways
4) Route Table
5) Network ACL (NACLs)
6) Security Grop



Benefits 
- Easy to use
- no additional changes 

Data can move from VPC to S3 but would need a VPC endpoint
Amazon (Elastic IP) - your public static IP - is billed even when the resource is powered off. Will still be billed until removed from account
not using an IP address but keeping it with you = IP address parking 
Public IP - also public but dynamic

VPC - Peering
communication between two different VPC (this is not depenedent of region or accounts)
Intra region , Cross Region , Cross Account

Intra region is not paid everything else is 

VPN = Site to SITE or dialup VPN
amazon can provide managed gateway as well but this is paid

VPC Flow logs - need to be enabled is not default 
somehting that gives you logs of what is happening on the network 

CIDR is valid for a  Region  (min is /28 and max is a /16)
Availability zones have Subnets for that region
Subnets can not span to a different Availability zones
to move a machine to a different AZ you would need to move it to a different VPC as well

Route table is not for a subnet but the entire CIDR together
igw-id = internet gateway ID
vgw-id = virtual private gateway ID

Bestian host != Jump server in amazon
jump host = a landing server within the corporate access for corporate employees who are also on some company LAN 
bastion host = a landing server for people on the internet to use some service on the private network of some company , not for managing stuff

Every subnet has 5 IPs reseved
1) network address  10.0.0.0/24
2) The first IP is reserved for the gateway 10.0.0.1/24
3) The second IP for the DNS is reserved 10.0.0.2/24
4) Reserved for future use 10.0.0.3/24
5) broadcast address 10.0.0.255/24


Security Group = is attached to a resource and is valid for instance based. Basically your stateful ACL for your EC2. Has implicit deny at the end which is hidden
hence only supports allow rules 
all rules are checked before deciding if traffic is allowed because the deny is at the end of code and can not be in between the ACL sequence
inbound rule = traffic to the EC2 instance
outboud rule = traffic from the EC2 instance



NACL - same as above but this is not stateful and works at subnet level (one NACL per subnet)
if not assigned manually the default NACL is assiged forecfully
NACL is for in-bound rules and outbound rule separately  (these are stateless)
rule entries (1-32766) top down ACL matches rule with implicit deny at the end
Defauly NACL is permit ip any any

Ephermeral Ports : random port numbers my endpoint uses to initiate traffic for the web server etc
This depends on the type of OS i have
NACL is per subnet but you can have a single NACL associated with more than 1 subnet
NACL supports allow and deny rules






a VPN Gateway is assigned or attached to a subnet in the subnet
will not work with a different subnet in the same CIDR if they don't have a VPN gateway . obvoiusly can use the jump host logic

NAT gateway = a machine that will basically NAT your priviate IP . Does something like what your home internet router does (it does PAT)
Since it is doing PAT , no can make a connection to your private machine or resources 


public NAT gateway - PAT
private NAT gateway - for S2S VPN in case your IP overlaps with your partner
NAT gateway =  AWS managed NAT, higher bandwidth , high Availability not administration
NAT gateway must be in a different subnet then the one who is being natted so (private subnet -> NAT Gateway -> IGW)
pay per hour of usage and bandwidth
createad in a specific AZ and uses an elastic IP
initially 5G bandwidth scaling upto 45 Gbps
no security group to managed
works within a single AZ , need to have multiple GW in multiple AZ to have fault tolerance
resilient within the same AZ Only


VPC endpoint (AWS PrivateLink)
They are redundant and scale horizontally
A VPC endpoint is a private connection that allows resources within your VPC to securely access AWS services without going through the public internet. It acts as a direct entry point from your VPC to the specific AWS service, bypassing the need for internet connectivity.
This is the use case for two resources to communicate between two resources
A sort of connection between two services privately 
normally EC2 -> NAT GW -> IGW -> Amazon SNS


Interface Endpoint - elastic network interface , creates a network interface dedicatedly with that service. 
Traffic does not go to gateway , exisits the newly created interface (must use a Security group since a new network interface is introducted) 
Cost per hour and per GB of data processed




Gateway Endpoint - entry in a route table to access services outside supports S3 and Dynamo DB at the moment (free)
Better to use gateway endpoint for VPC to S3 but better to use Interface Endpoint between S2S VPN or Direction Connect to S3

Gateway load balancer endpoint - routes traffic to a GLB


AWS PrivateLink can be used to create VPC endpoints for AWS services, as well as for connecting to services provided by AWS partners. 
It provides a consistent and secure way to access both AWS and third-party services from within your VPC.

To summarize, VPC endpoints are specific instances that enable private access to AWS services from your VPC, while AWS PrivateLink is the underlying technology that facilitates secure and private connectivity between your VPC and AWS services or AWS partner services.


VPC Peering
For communication between two different VPC (can be in same/different account or in same/different regions)
Limitation 
1) overlapping CIDR
2) Transitive peering (A is peer to B and B is peer to C) A will not communicate to C
3) edge to edge (gateway communication will not use the peer link)


Referencing a security group in a peer VPC means that you can specify the security group ID from the other VPC when defining the inbound or outbound rules in a security group. This allows you to control and regulate the traffic flow between instances in different VPCs using the familiar security group rules.

By referencing a security group in a peer VPC, you can simplify network security management and ensure consistent security policies across multiple VPCs. 
It allows you to define and manage access rules between VPCs in a centralized manner using security groups, providing granular control over the communication between instances in different VPCs while maintaining a secure network environment.

Reachability Analyzer : static configuration analysis tool , that tells you the hop by hop by Detailed
VPC Traffic monitoring : basically your mirroring to use wireshark later

Transit Gateway: Basically the solutuin to the Limitation of VPC peering. But not limited to that , you can use it as a randevou point for your VPN connections as well
Network Firewall: Amazon's own firewall , can do IPS and IDS as well


Max VPC per region = 5
max 4 secondary IP per VPC (min is /28 - max is /16)
200 subnet per amazon VPC
5 elastic IP address per AWS account per region
10 hardware connection per amazon VPC
 



What is Tenancy in VPC?
the hardware that my resource would be located on whould be Dedicated for me or shared


Default vs Dedicated ?


Resource-based name (RBN) vs IP address-based naming (IPBN) ?

NAT instance works in such a way that it is the gateway for the private subnet and it simply nats the traffic and exists
NAT instance needs to be in public subnet to have a public IP and a default route
NAT instance needs to be able to work as a transit for clients traffic

Pre configured linux AMI available for NAT instance
reached end of support on december 31 , 2020
not highly available: You would need to create an ASG in multi-AZ + resilient user-data script
Inernet traffic bandwidth depends on EC2 instance type







EC2 

Lifecycle Policy = basically management of the snapshots

the compute engine of th AWS which user a server based method
The capacity of the EC2 is Flexible (Scale up and down) - Vertical scaling
User has root access of the entire machine
Templates are available some are community based , some are AWS based , all available in the marketplace
virtual machine is called = instancce
bootable images are placed in the market placed = AMI (amazon machine images) or golden AMI (fresh AMI , panni pack)

Instance Type = The type of sepcfic instance type you need as per your requirements e.g memory optimized etc etc. These are selected in a bundle
Bundle (CPU , memeory storage etc)

EC2 = stop and start changes the phyiscal hardware everytime 
EBS Volume = persistant storage class (the memeory in the EC2) = mounted on the EC2 through the network
EC2 is not a global service
SG = Security Group is assigned on a EC2

Instance Store Volumnes = Memory that deletes when an instance stops , can be used for buffers , caches , temp data etc.
This is ephemeral storage , which is the storage of the hardward on which your machine is on
used for better IO performanace 
This comes attached to a specific type of instance only and can not be attached to an instance with an EBS 


Hibernation = pausing a machine or making it sleep (just like a normal PC)
Stop Instance = when you stop an instance it is detached from the phyiscal hardware and 
when you turn it on it might go on to a different bare metal (host)

General Prpose
Compute optimized (for CPU optimized)
Memory optimized
Storage optimized
Accelerated computing (for CPU and Memory hungery apps)


AMI = basic image of an OS , which will allow you to change stuff and create your own AMI as well from a base AMI
AMI snapshot are incremental (e.g an AMI snapshot from another AMI will not have the whole volume but a delta only)
first snapshot is the whole snapshot (e.g 8GB) and the second snapshot is the only the delta (e.g 1 GB)

if you delete the first snapshot it will first copy the data into the second snapshot before deleting the orignal snapshot
so now your second snapshot will be 9 GB

AMI uses S3 as root volume
AMI uses EBS as data volume


EBS are replicated within the same AZ as well 
EBS (HDD or SDD are available with an Availability of 99.999%)
EBS (Encryption available for data at rest and data in transit)
Allows taking snapshots
a sepatate access management can be maintained

Elastic Volume (can only increase not decrease) can be modified on the fly


IOPS = number of read write on the volume 16000 this 1:4 to the size of your storage e.g if you have 34 GB of memory you will get 102 IOPs
Throughput = total amount of traffic that can pass through e.g 1000 MiB/s
Volume = 1G to 16TB

General purpose SSD vs Provisioned IOPS SSD ?

Provisioned IOS SSD = supports multi attach EBS
To have best value , start with minimum value in General Pupose SSD and if you feel throtling or chocking , move class then 
(this is on fly)



On Demand Vs Spot Instance

On Demand = billed per hour or per hour of uptime
Spot Instance = Resources that were resrved for dedicated or on demand but are not being used but you will need to give up those when the actual customer arrives
savings Plans : a contract that allows you lower price for up to 72% discoount if you sign a contract for an X number of hours in the years
Reserved Instance : same as saving plan but you are reserved for an instance type as well e.g T2Micro etc . 
In AWS, Reserved Instances (RIs) are a billing option that allows you to purchase EC2 instances for a specified term (1 or 3 years) at a significantly discounted hourly rate compared to On-Demand instances.
Dedicated Hosts: a physical EC2 server 

AWS EC2 machines are billed per second




Placement Group = Allows you to launce instances physically near each other 

AMI vs Snapshot?

AMI = image when you need a bootable image backup
Snapshot = a Snapshot is normally created of an additional storage that can be attached to some other service , 
a volume or an AMI can be created from a snapshot. A snapshot can be publicly or account shared as well.

A template is just a pre checked pointers e.g subnet and machine type etc so you can just click "launch" and the config template that you had used before 
This is most used in load balancing or in case of auto  scaling because you will need the new machines to have the exact same specs 


In simple terms, an AMI is used to create a new virtual server from scratch, while a snapshot is used to make a copy of the data on a volume attached to an existing server.

<> EC2 Image Builder

Used for creating or modifying or automating the creation of custom AMI from a golden AMI
Steps:
Provide a base image
select a software of installation
select and run tests
distribute images to selected regions

Automates pipeline to keep images up to date
allows sharing within an account or cross account
supports EC2 and containers as well

<> ELB (Elastic Load balancer)
Binding is based on endpoint
used with EC2
exactly what it sounds like (just a plain old LB that distributes traffic between multiple VM in multiple AZ)
taget groups are called a group of compute machines but they need to be in the same AZ
 targets are called machines where the traffic is suppose to land


health check - can monitor a node or target to check if they are healty and can receive traffic
Security - Certificate can be bound on the LB and internally without encryption can be flowed
ELB can do an SSL offload as well

classis load balancer will handle ip based traffic load balancing only
can do sticky persistancy 


Application load balancer works on application basis 
can have multi instance groups and policies can be defined as per listner rules

Network loadbalancer
binding is based on transport level (TCP and UDP)

GLB (gateway load balancer)
binding is based on IP address only
works with VPC endpoints 

<> Auto scaling

desired capacity - start with two
minimum capacity - should have at least this many
maximum capacity - do not exceed with this


changing the specs of a machine is called vertical scaling
adding more instance is called horizontal scaling

alters the horizontal scaling as per metrics set previously
auto scaling and loadbalancer are better to be used in combination but are independt of each other.
if working with ELB it will also add the new ec2 in the load balancer

scale out - increase number of machines
scale in - restore to original number

terminates un-healthy interface and brings up a new healthy one = this is auto healing

launch configuration = a template of the auto scaling , here is where you need to define how do you want to scale up e.g AMI type , Security group etc.

launch config vs template?

life cycle hooks = actions into an instance before it goes into service or after it goes into termantion 

target tracking = increase or decrease capacity of the group based on this value for a specific metric
step scaling - how many instances to increase when thresholds exceed e.g increase 2 instance per step
simple scaling - what we have been doing for now
scheduled scaling - schedule and up-scale or down-scale
predictive scaling - use machine learning to schedule the right number of instance and inticpate approaching traffic

<> Route 53 (DNS web service)

This is nothing more than a DNS server
This can be your domain management as well

allows to create policies that are time based as well

- simple routing policy = your A amd MX records
- failover routing policy = $50 for 1 policy - allows to create a faiover to another record if the primary one/host fails. This is not wrt to an AZ , can be out of region as well
(this is done by using it's own healt checks)

- Geo location routing policy - route traffic of users of EU to servers in the EU
- Geo proximity routing policy - shift traffic from resources in one location to another based on the best experience of the user. The hardware capacity is concisdered which will be best for user
- Latecny routing policy - routes you to nearest server which have lowest latency
- IP based roting - route specific prefix to specific servers
- multi-value answer routing policy - if you have say 8 servers , in multi value it will respond with an A record of any of the 8 records
- weighted routing policy - send 60% traffic to here and 40 % to there, basically on based on weight
- Priavate DNS for amazon VPC - your private DNS , that works as long as your are on that specific network
- Health Checks and monitoring - this is how it checks if primary server is up or not

- Domain Registeration = allows you to create a domain and manage it from AWS as well
- S3 zone apex support -



<> Event Bridge
for event driven architecture
for an action to follow an event (e.g object created in an event)
1 event can have upto 5 different targets
Event Producure can be anything like an S3 or EC2
Event Bus - Rule based on what to do when event happens
Targets - on where you want the action to happen

feature
Sechduling
Monitoring and auditng
Pay per event (Event generated by AWS services are free but you pay for event for you own apps , e.g SaaS)

<> lambda
Regional Services
let's you run code without any backend service
Basically this is your serverless environment


Create a code ,
upload a code
setup a triger to run a code

E.g a code to re-size an image to upload some place
E.g you upload a file and the code converts you file in the respective size and uploads it on your code or whatever

Benefits
serverless
continous scaling
sub-second meetring

lambad function
The code you run on your AWS lambda
each function has some associated config info to it
they are stateless = it is not depending on a queue or anything else. it does not have a state and will run on it's own in parallel
can be used with multiple services as well


- supports bring your own code (code in .net 6 , java etc)
- you can only choose the memory that you want to assosiate , the cpu network and disk IO will be assigned by AWS


Lambda can access other services as well
can be run into a VPC as well but not recommended

Limit = 1000 fuctions per region (soft limit , can be increated)
resource limit = /tmp (512MB- 10240)
max threates = 1024
max execurtion per request = 900 seconds (will timeout after that) 
upto 5 lambda functions for a single event
max eni = 250
memory limit = 128 MB to 10240 MB with 1 MB increments


<> SQS
Simple queue Serivce
it's a type of buffer or temp repository of messages
producres produces a message to send to consumer (your consumer is your consumer of the request which is actually doing the compute or process on your message)
with SQS this becaomes a tightly copuled architecture

message queueing service = something like a middleware
just a middleware that holds messages and builds a queue before forwarding it to the processor or some other sort of service
if the same message keeps coming from the producer but it is not being catered , it can be sent to a different queue called DLQ 
this helps your compute or your consumer for additional requests are continous and can not be processed 
once the consumer picks up the message a back visibility timeout is set for it so another consumer does not pick it up to process
e.g a video upload system and a process of creating a thimbnail could use this

long polling vs short polling 
long polling - picks up message for a lnger time and picks up a 



LAB Pending
Create AMI & Snapshot.
Create Lifecycle Policy
Create EC2 Template 
